def optimize(X, Y, w, b, num_iterations, learning_rate, propagation_fun, print_cost=False):
    costs = []
    for i in range(num_iterations):

        # Cost and gradient calculation (â‰ˆ 1-4 lines of code)
        grads, cost = propagation_fun(w, b, X, Y)

        # Retrieve derivatives from grads
        dw = grads["dw"]
        db = grads["db"]

        w = w - learning_rate * dw
        b = b - learning_rate * db

        # Record the costs
        if i % 100 == 0:
            costs.append(cost)

        # Print the cost every 100 training examples
        if print_cost and i % 100 == 0:
            print("Cost after iteration %i: %f" % (i, cost))

    params = {"w": w,
              "b": b}

    return params, costs
